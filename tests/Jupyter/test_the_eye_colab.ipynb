{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# The Eye - SOM Quality Prediction CNN\n",
    "\n",
    "This notebook allows you to test \"The Eye\" CNN model for predicting Self-Organizing Map (SOM) quality.\n",
    "\n",
    "**The Eye** is a CNN trained to predict SOM quality by analyzing RGB composite visualizations:\n",
    "- **Red Channel**: U-Matrix (cluster boundaries)\n",
    "- **Green Channel**: Distance Map (quantization error)\n",
    "- **Blue Channel**: Dead Neurons Map\n",
    "\n",
    "## Quick Start\n",
    "1. Upload your trained model (.keras file)\n",
    "2. Upload RGB SOM maps or test set CSV\n",
    "3. Run evaluation cells\n",
    "4. Download predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "imports",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1768323016835,
     "user_tz": -60,
     "elapsed": 9879,
     "user": {
      "displayName": "Tomas Kotasek",
      "userId": "15797747045199155999"
     }
    },
    "outputId": "bed71055-5062-45b9-dcbe-ae99b86df086",
    "ExecuteTime": {
     "end_time": "2026-01-14T09:14:25.580247Z",
     "start_time": "2026-01-14T09:14:25.537951Z"
    }
   },
   "source": [
    "import os\n",
    "import sys\n",
    "import platform\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "# 1. Identifikace Rootu a Cest\n",
    "project_root = Path(os.getcwd()).resolve()\n",
    "model_path = project_root / \"som_quality.keras\"\n",
    "\n",
    "print(f\"{' PROJECT STATUS ':=^80}\")\n",
    "print(f\"Root:      {project_root}\")\n",
    "print(f\"OS:        {platform.system()} {platform.release()} ({platform.machine()})\")\n",
    "print(f\"Python:    {sys.version.split()[0]} ({sys.executable})\")\n",
    "\n",
    "# 2. Verze klíčových knihoven\n",
    "print(f\"\\n{' LIBRARIES ':-^80}\")\n",
    "print(f\"TensorFlow: {tf.__version__}\")\n",
    "print(f\"Keras:      {keras.__version__}\")\n",
    "print(f\"NumPy:      {np.__version__}\")\n",
    "\n",
    "# 3. Hardware / Accel\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "gpu_status = f\"ACTIVE ({gpus[0].name})\" if gpus else \"INACTIVE (CPU only)\"\n",
    "print(f\"Metal GPU:  {gpu_status}\")\n",
    "\n",
    "# 4. Diagnostika problému s načítáním\n",
    "print(f\"\\n{' MODEL DIAGNOSTICS ':-^80}\")\n",
    "print(f\"Soubor existuje: {model_path.exists()}\")\n",
    "if model_path.exists():\n",
    "    print(f\"Velikost:        {model_path.stat().st_size / (1024*1024):.2f} MB\")\n",
    "\n",
    "# Detekce potenciálního konfliktu Keras 2 vs 3\n",
    "is_keras_3 = hasattr(keras, \"ops\")\n",
    "print(f\"Keras engine:    {'Keras 3 (Modern)' if is_keras_3 else 'Keras 2 (Legacy)'}\")\n",
    "print(f\"{'':=^80}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================ PROJECT STATUS ================================\n",
      "Root:      /Users/tomas/OSU/Python/NexusSom/tests/Jupyter\n",
      "OS:        Darwin 25.0.0 (arm64)\n",
      "Python:    3.11.14 (/Users/tomas/OSU/Python/NexusSom/.venv/bin/python)\n",
      "\n",
      "---------------------------------- LIBRARIES -----------------------------------\n",
      "TensorFlow: 2.18.0\n",
      "Keras:      3.13.0\n",
      "NumPy:      1.26.4\n",
      "Metal GPU:  ACTIVE (/physical_device:GPU:0)\n",
      "\n",
      "------------------------------ MODEL DIAGNOSTICS -------------------------------\n",
      "Soubor existuje: True\n",
      "Velikost:        14.75 MB\n",
      "Keras engine:    Keras 3 (Modern)\n",
      "================================================================================\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upload_model"
   },
   "source": [
    "## 2. Upload Model\n",
    "\n",
    "Upload your trained `.keras` model file."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "upload_model_code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 671
    },
    "executionInfo": {
     "status": "error",
     "timestamp": 1768323708393,
     "user_tz": -60,
     "elapsed": 78048,
     "user": {
      "displayName": "Tomas Kotasek",
      "userId": "15797747045199155999"
     }
    },
    "outputId": "b6fe8236-aa86-4565-8ef4-fc0881341bca"
   },
   "source": [
    "from pathlib import Path\n",
    "import keras\n",
    "\n",
    "# Použití Pathlib pro robustnost\n",
    "model_path = Path(\"som_quality.keras\").resolve()\n",
    "\n",
    "if not model_path.exists():\n",
    "    raise FileNotFoundError(f\"Model nebyl nalezen na adrese: {model_path}\")\n",
    "\n",
    "try:\n",
    "    # V Kerasu 3 je .keras nativní a doporučený formát\n",
    "    model = keras.models.load_model(model_path)\n",
    "    print(f\"✓ Model '{model_path.name}' úspěšně načten.\")\n",
    "except ValueError as e:\n",
    "    # Specifická chyba pro Keras 3 (např. custom layers)\n",
    "    print(f\"⚠ Chyba při načítání (pravděpodobně custom objects): {e}\")\n",
    "    # Zde bys definoval custom_objects={'TvojeVrstva': TvojeVrstva}\n",
    "except Exception as e:\n",
    "    print(f\"✗ Kritická chyba: {e}\")\n",
    "\n",
    "model.summary()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "helper_functions"
   },
   "source": [
    "## 3. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "helper_code"
   },
   "source": [
    "def load_and_preprocess_image(filepath, image_size=(224, 224)):\n",
    "    \"\"\"Load and preprocess a single image\"\"\"\n",
    "    try:\n",
    "        img = Image.open(filepath).convert('RGB')\n",
    "        img = img.resize(image_size, Image.LANCZOS)\n",
    "        img_array = np.array(img, dtype=np.float32) / 255.0\n",
    "        return img_array\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image {filepath}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def predict_single_image(model, image_path, image_size=(224, 224), threshold=0.5):\n",
    "    \"\"\"Predict quality for a single image\"\"\"\n",
    "    img = load_and_preprocess_image(image_path, image_size)\n",
    "    if img is None:\n",
    "        return None\n",
    "\n",
    "    img_batch = np.expand_dims(img, axis=0)\n",
    "    prediction = model.predict(img_batch, verbose=0)[0][0]\n",
    "\n",
    "    quality_label = \"GOOD\" if prediction >= threshold else \"BAD\"\n",
    "    confidence = prediction if prediction >= 0.5 else (1 - prediction)\n",
    "\n",
    "    return {\n",
    "        'quality_score': prediction,\n",
    "        'quality_label': quality_label,\n",
    "        'confidence': confidence\n",
    "    }\n",
    "\n",
    "\n",
    "def visualize_prediction(image_path, prediction_result):\n",
    "    \"\"\"Visualize image with prediction\"\"\"\n",
    "    img = Image.open(image_path)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Add prediction text\n",
    "    score = prediction_result['quality_score']\n",
    "    label = prediction_result['quality_label']\n",
    "    confidence = prediction_result['confidence']\n",
    "\n",
    "    color = 'green' if label == 'GOOD' else 'red'\n",
    "    title = f\"Quality: {label}\\nScore: {score:.4f} | Confidence: {confidence:.2%}\"\n",
    "\n",
    "    plt.title(title, fontsize=14, weight='bold', color=color, pad=10)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def calculate_metrics(predictions, labels, threshold=0.5):\n",
    "    \"\"\"Calculate evaluation metrics\"\"\"\n",
    "    predictions = np.array(predictions)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    # Regression metrics\n",
    "    errors = np.abs(predictions - labels)\n",
    "    mae = np.mean(errors)\n",
    "    rmse = np.sqrt(np.mean((predictions - labels) ** 2))\n",
    "\n",
    "    # Classification metrics\n",
    "    pred_classes = (predictions >= threshold).astype(int)\n",
    "    true_classes = (labels >= threshold).astype(int)\n",
    "\n",
    "    accuracy = np.mean(pred_classes == true_classes)\n",
    "    tp = np.sum((pred_classes == 1) & (true_classes == 1))\n",
    "    fp = np.sum((pred_classes == 1) & (true_classes == 0))\n",
    "    fn = np.sum((pred_classes == 0) & (true_classes == 1))\n",
    "    tn = np.sum((pred_classes == 0) & (true_classes == 0))\n",
    "\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    return {\n",
    "        'mae': mae,\n",
    "        'rmse': rmse,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'confusion_matrix': {'tp': tp, 'fp': fp, 'tn': tn, 'fn': fn}\n",
    "    }\n",
    "\n",
    "print(\"✓ Helper functions loaded!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "option_select"
   },
   "source": [
    "## 4. Choose Testing Mode\n",
    "\n",
    "Select one of the following:\n",
    "- **Option A**: Test on a single RGB image\n",
    "- **Option B**: Test on multiple images (ZIP file)\n",
    "- **Option C**: Evaluate on test set (CSV with filepaths and labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "option_a"
   },
   "source": [
    "### Option A: Test Single Image"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "option_a_code"
   },
   "source": [
    "# Upload a single RGB SOM image\n",
    "if not IN_COLAB:\n",
    "    print(\"⚠ Not in Colab - please set img_filename manually\")\n",
    "    img_filename = \"example_rgb.png\"  # Change this to your image path\n",
    "else:\n",
    "    print(\"Please upload a single RGB SOM image (PNG format)...\")\n",
    "    uploaded_img = files.upload()\n",
    "\n",
    "    # Get the uploaded image filename\n",
    "    img_filename = list(uploaded_img.keys())[0]\n",
    "    print(f\"\\n✓ Image uploaded: {img_filename}\")\n",
    "\n",
    "# Predict\n",
    "print(\"\\nMaking prediction...\")\n",
    "result = predict_single_image(model, img_filename)\n",
    "\n",
    "if result:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"PREDICTION RESULT\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Quality Score: {result['quality_score']:.6f}\")\n",
    "    print(f\"Quality Label: {result['quality_label']}\")\n",
    "    print(f\"Confidence: {result['confidence']:.2%}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "    # Visualize\n",
    "    visualize_prediction(img_filename, result)\n",
    "else:\n",
    "    print(\"✗ Failed to process image\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "option_b"
   },
   "source": [
    "### Option B: Test Multiple Images (ZIP)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "option_b_upload"
   },
   "source": [
    "# Upload a ZIP file containing RGB images\n",
    "if not IN_COLAB:\n",
    "    print(\"⚠ Not in Colab - please set zip_filename manually\")\n",
    "    zip_filename = \"images.zip\"  # Change this to your ZIP path\n",
    "else:\n",
    "    print(\"Please upload a ZIP file containing RGB SOM images...\")\n",
    "    uploaded_zip = files.upload()\n",
    "\n",
    "    # Get the uploaded ZIP filename\n",
    "    zip_filename = list(uploaded_zip.keys())[0]\n",
    "    print(f\"\\n✓ ZIP uploaded: {zip_filename}\")\n",
    "\n",
    "# Extract ZIP\n",
    "extract_dir = \"uploaded_images\"\n",
    "os.makedirs(extract_dir, exist_ok=True)\n",
    "\n",
    "with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_dir)\n",
    "\n",
    "print(f\"✓ Extracted to: {extract_dir}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "option_b_predict"
   },
   "source": [
    "# Find all PNG images\n",
    "image_files = []\n",
    "for root, dirs, files_list in os.walk(extract_dir):\n",
    "    for file in files_list:\n",
    "        if file.endswith('.png'):\n",
    "            image_files.append(os.path.join(root, file))\n",
    "\n",
    "print(f\"Found {len(image_files)} PNG images\\n\")\n",
    "\n",
    "if len(image_files) == 0:\n",
    "    print(\"✗ No PNG images found in ZIP file\")\n",
    "else:\n",
    "    # Predict on all images\n",
    "    results = []\n",
    "\n",
    "    print(\"Making predictions...\")\n",
    "    for i, img_path in enumerate(image_files):\n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"  Progress: {i+1}/{len(image_files)}\")\n",
    "\n",
    "        result = predict_single_image(model, img_path)\n",
    "        if result:\n",
    "            results.append({\n",
    "                'filename': os.path.basename(img_path),\n",
    "                'filepath': img_path,\n",
    "                'quality_score': result['quality_score'],\n",
    "                'quality_label': result['quality_label'],\n",
    "                'confidence': result['confidence']\n",
    "            })\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df = results_df.sort_values('quality_score', ascending=False)\n",
    "\n",
    "    # Summary\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"PREDICTION SUMMARY\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "    good_count = (results_df['quality_score'] >= 0.5).sum()\n",
    "    bad_count = (results_df['quality_score'] < 0.5).sum()\n",
    "\n",
    "    print(f\"Total predictions: {len(results_df)}\")\n",
    "    print(f\"  Good (score >= 0.5): {good_count}\")\n",
    "    print(f\"  Bad (score < 0.5): {bad_count}\")\n",
    "\n",
    "    print(f\"\\nTop 10 Best Quality Maps:\")\n",
    "    print(results_df.head(10)[['filename', 'quality_score', 'quality_label', 'confidence']].to_string(index=False))\n",
    "\n",
    "    print(f\"\\nTop 10 Worst Quality Maps:\")\n",
    "    print(results_df.tail(10)[['filename', 'quality_score', 'quality_label', 'confidence']].to_string(index=False))\n",
    "\n",
    "    # Save results\n",
    "    output_csv = \"predictions.csv\"\n",
    "    results_df.to_csv(output_csv, index=False)\n",
    "    print(f\"\\n✓ Predictions saved to: {output_csv}\")\n",
    "\n",
    "    # Download results\n",
    "    if IN_COLAB:\n",
    "        print(\"\\nDownloading predictions.csv...\")\n",
    "        files.download(output_csv)\n",
    "    else:\n",
    "        print(f\"\\n✓ Results saved locally to {output_csv}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "option_b_visualize"
   },
   "source": [
    "# Visualize sample predictions\n",
    "print(\"Visualizing sample predictions...\\n\")\n",
    "\n",
    "# Show top 3 best and top 3 worst\n",
    "sample_indices = list(results_df.head(3).index) + list(results_df.tail(3).index)\n",
    "\n",
    "for idx in sample_indices:\n",
    "    row = results_df.iloc[idx]\n",
    "    result = {\n",
    "        'quality_score': row['quality_score'],\n",
    "        'quality_label': row['quality_label'],\n",
    "        'confidence': row['confidence']\n",
    "    }\n",
    "    print(f\"\\nImage: {row['filename']}\")\n",
    "    visualize_prediction(row['filepath'], result)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "option_c"
   },
   "source": [
    "### Option C: Evaluate on Test Set (CSV)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "option_c_upload"
   },
   "source": [
    "# Upload test set CSV\n",
    "if not IN_COLAB:\n",
    "    print(\"⚠ Not in Colab - please set csv_filename manually\")\n",
    "    csv_filename = \"test_set.csv\"  # Change this to your CSV path\n",
    "else:\n",
    "    print(\"Please upload test set CSV file (must have 'filepath' and 'quality_score' columns)...\")\n",
    "    uploaded_csv = files.upload()\n",
    "\n",
    "    # Get the uploaded CSV filename\n",
    "    csv_filename = list(uploaded_csv.keys())[0]\n",
    "    print(f\"\\n✓ CSV uploaded: {csv_filename}\")\n",
    "\n",
    "# Load test set\n",
    "test_df = pd.read_csv(csv_filename)\n",
    "print(f\"\\nTest set size: {len(test_df)} samples\")\n",
    "print(f\"Columns: {list(test_df.columns)}\")\n",
    "\n",
    "if 'filepath' not in test_df.columns or 'quality_score' not in test_df.columns:\n",
    "    print(\"\\n✗ Error: CSV must contain 'filepath' and 'quality_score' columns\")\n",
    "else:\n",
    "    print(\"\\n✓ Test set loaded successfully!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "option_c_upload_images"
   },
   "source": [
    "# Upload images referenced in the CSV\n",
    "if not IN_COLAB:\n",
    "    print(\"⚠ Not in Colab - please set img_zip_filename manually\")\n",
    "    img_zip_filename = \"test_images.zip\"  # Change this to your ZIP path\n",
    "else:\n",
    "    print(\"Please upload a ZIP file containing the images referenced in the CSV...\")\n",
    "    uploaded_img_zip = files.upload()\n",
    "\n",
    "    # Get the uploaded ZIP filename\n",
    "    img_zip_filename = list(uploaded_img_zip.keys())[0]\n",
    "    print(f\"\\n✓ ZIP uploaded: {img_zip_filename}\")\n",
    "\n",
    "# Extract ZIP\n",
    "images_dir = \"test_images\"\n",
    "os.makedirs(images_dir, exist_ok=True)\n",
    "\n",
    "with zipfile.ZipFile(img_zip_filename, 'r') as zip_ref:\n",
    "    zip_ref.extractall(images_dir)\n",
    "\n",
    "print(f\"✓ Extracted to: {images_dir}\")\n",
    "\n",
    "# Update filepaths in test_df to point to extracted location\n",
    "# Assumes images are directly in the ZIP root\n",
    "test_df['local_filepath'] = test_df['filepath'].apply(\n",
    "    lambda x: os.path.join(images_dir, os.path.basename(x))\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "option_c_evaluate"
   },
   "source": [
    "# Evaluate on test set\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"EVALUATING ON TEST SET\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Load images and labels\n",
    "images = []\n",
    "labels = []\n",
    "valid_indices = []\n",
    "\n",
    "print(\"Loading images...\")\n",
    "for idx, row in test_df.iterrows():\n",
    "    img = load_and_preprocess_image(row['local_filepath'])\n",
    "    if img is not None:\n",
    "        images.append(img)\n",
    "        labels.append(row['quality_score'])\n",
    "        valid_indices.append(idx)\n",
    "    else:\n",
    "        print(f\"  Warning: Could not load {row['local_filepath']}\")\n",
    "\n",
    "images = np.array(images)\n",
    "labels = np.array(labels)\n",
    "\n",
    "print(f\"\\nLoaded {len(images)} images\")\n",
    "\n",
    "# Predict\n",
    "print(\"\\nMaking predictions...\")\n",
    "predictions = model.predict(images, verbose=1)\n",
    "predictions = predictions.flatten()\n",
    "\n",
    "# Calculate metrics\n",
    "metrics = calculate_metrics(predictions, labels)\n",
    "\n",
    "# Print results\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"EVALUATION RESULTS\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "print(\"Regression Metrics:\")\n",
    "print(f\"  Mean Absolute Error (MAE): {metrics['mae']:.6f}\")\n",
    "print(f\"  Root Mean Squared Error (RMSE): {metrics['rmse']:.6f}\")\n",
    "\n",
    "print(f\"\\nClassification Metrics (threshold=0.5):\")\n",
    "print(f\"  Accuracy: {metrics['accuracy']:.4f} ({metrics['accuracy']*100:.2f}%)\")\n",
    "print(f\"  Precision: {metrics['precision']:.4f}\")\n",
    "print(f\"  Recall: {metrics['recall']:.4f}\")\n",
    "print(f\"  F1-Score: {metrics['f1']:.4f}\")\n",
    "\n",
    "cm = metrics['confusion_matrix']\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(f\"  True Positives: {cm['tp']}\")\n",
    "print(f\"  False Positives: {cm['fp']}\")\n",
    "print(f\"  True Negatives: {cm['tn']}\")\n",
    "print(f\"  False Negatives: {cm['fn']}\")\n",
    "\n",
    "# Show sample predictions\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"SAMPLE PREDICTIONS\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "for i in range(min(10, len(predictions))):\n",
    "    pred_label = \"GOOD\" if predictions[i] >= 0.5 else \"BAD\"\n",
    "    true_label = \"GOOD\" if labels[i] >= 0.5 else \"BAD\"\n",
    "    match = \"✓\" if pred_label == true_label else \"✗\"\n",
    "    print(f\"{match} Sample {i+1}: Predicted={predictions[i]:.4f} ({pred_label}), True={labels[i]:.1f} ({true_label})\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "option_c_visualize"
   },
   "source": [
    "# Visualize prediction distribution\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot 1: Prediction vs True label scatter\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(labels, predictions, alpha=0.5)\n",
    "plt.plot([0, 1], [0, 1], 'r--', label='Perfect prediction')\n",
    "plt.axhline(y=0.5, color='orange', linestyle='--', alpha=0.5, label='Threshold')\n",
    "plt.axvline(x=0.5, color='orange', linestyle='--', alpha=0.5)\n",
    "plt.xlabel('True Quality Score')\n",
    "plt.ylabel('Predicted Quality Score')\n",
    "plt.title('Predictions vs True Labels')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Prediction distribution\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(predictions[labels == 1.0], bins=20, alpha=0.5, label='True GOOD', color='green')\n",
    "plt.hist(predictions[labels == 0.0], bins=20, alpha=0.5, label='True BAD', color='red')\n",
    "plt.axvline(x=0.5, color='black', linestyle='--', label='Threshold')\n",
    "plt.xlabel('Predicted Quality Score')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Prediction Distribution')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "conclusion"
   },
   "source": [
    "## 5. Model Performance Summary\n",
    "\n",
    "**The Eye** CNN Model Capabilities:\n",
    "- Predicts SOM quality from RGB composite visualizations\n",
    "- Outputs quality score (0.0 = BAD, 1.0 = GOOD)\n",
    "- High precision (avoids false positives)\n",
    "- Can be integrated into evolutionary algorithms for real-time quality assessment\n",
    "\n",
    "### RGB Channel Interpretation:\n",
    "- **Red**: U-Matrix (cluster boundaries - dark=similar, yellow=boundaries)\n",
    "- **Green**: Distance Map (quantization error)\n",
    "- **Blue**: Dead Neurons Map (unused neurons)\n",
    "\n",
    "### Quality Indicators:\n",
    "- Clear cluster boundaries in U-Matrix\n",
    "- Low quantization error (distance map)\n",
    "- Few dead neurons\n",
    "- Well-organized topology\n",
    "\n",
    "---\n",
    "\n",
    "**For more information:**\n",
    "- GitHub: NexusSom Project\n",
    "- Model trained on evolutionary algorithm (EA) results\n",
    "- Fixed [0, 1.0] normalization for consistent visual interpretation"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
