{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# The Eye - SOM Quality Prediction CNN\n",
    "\n",
    "This notebook allows you to test \"The Eye\" CNN model for predicting Self-Organizing Map (SOM) quality.\n",
    "\n",
    "**The Eye** is a CNN trained to predict SOM quality by analyzing RGB composite visualizations:\n",
    "- **Red Channel**: U-Matrix (cluster boundaries)\n",
    "- **Green Channel**: Distance Map (quantization error)\n",
    "- **Blue Channel**: Dead Neurons Map\n",
    "\n",
    "## Quick Start\n",
    "1. Upload your trained model (.keras file)\n",
    "2. Upload RGB SOM maps or test set CSV\n",
    "3. Run evaluation cells\n",
    "4. Download predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_deps"
   },
   "outputs": [],
   "source": "# Install required packages - upgrade to latest TensorFlow for compatibility\n!pip install -q --upgrade pip\n!pip install -q --upgrade tensorflow>=2.18.0 keras>=3.6.0 pandas numpy pillow matplotlib scikit-learn\n\nprint(\"✓ Dependencies installed successfully!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports"
   },
   "outputs": [],
   "source": "# Import libraries\nimport os\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom google.colab import files\nimport zipfile\nimport io\n\nprint(f\"TensorFlow version: {tf.__version__}\")\nprint(f\"Keras version: {keras.__version__}\")\nprint(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")\n\n# Check if TensorFlow version is compatible\ntf_version = tf.__version__\nmajor, minor = map(int, tf_version.split('.')[:2])\nif major < 2 or (major == 2 and minor < 18):\n    print(f\"\\n⚠ WARNING: TensorFlow {tf_version} detected.\")\n    print(\"  This model requires TensorFlow 2.18+\")\n    print(\"  Please restart runtime and run the installation cell again.\")\nelse:\n    print(f\"\\n✓ TensorFlow version compatible ({tf_version})\")\n\nprint(\"\\n✓ Imports successful!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upload_model"
   },
   "source": [
    "## 2. Upload Model\n",
    "\n",
    "Upload your trained `.keras` model file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upload_model_code"
   },
   "outputs": [],
   "source": "print(\"Please upload your trained model (.keras file)...\")\nuploaded = files.upload()\n\n# Get the uploaded model filename\nmodel_filename = list(uploaded.keys())[0]\nprint(f\"\\n✓ Model uploaded: {model_filename}\")\n\n# Load the model with compatibility handling\nprint(\"Loading model...\")\ntry:\n    # Try loading normally first\n    model = keras.models.load_model(model_filename)\n    print(\"✓ Model loaded successfully!\")\nexcept Exception as e:\n    print(f\"⚠ Standard loading failed: {e}\")\n    print(\"\\nTrying compatibility mode (loading weights only)...\")\n    \n    try:\n        # Alternative: Load with compile=False to skip optimizer state\n        model = keras.models.load_model(model_filename, compile=False)\n        \n        # Recompile with simple optimizer\n        model.compile(\n            optimizer='adam',\n            loss='mse',\n            metrics=['mae']\n        )\n        print(\"✓ Model loaded in compatibility mode (weights only)\")\n        print(\"  Note: Original optimizer state not loaded, using default Adam\")\n    except Exception as e2:\n        print(f\"✗ Compatibility mode also failed: {e2}\")\n        print(\"\\nPlease ensure you have TensorFlow 2.18+ installed:\")\n        print(\"  !pip install --upgrade tensorflow>=2.18.0\")\n        raise\n\n# Show model summary\nprint(\"\\n\" + \"=\"*80)\nprint(\"MODEL ARCHITECTURE\")\nprint(\"=\"*80)\nmodel.summary()"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "helper_functions"
   },
   "source": [
    "## 3. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "helper_code"
   },
   "outputs": [],
   "source": [
    "def load_and_preprocess_image(filepath, image_size=(224, 224)):\n",
    "    \"\"\"Load and preprocess a single image\"\"\"\n",
    "    try:\n",
    "        img = Image.open(filepath).convert('RGB')\n",
    "        img = img.resize(image_size, Image.LANCZOS)\n",
    "        img_array = np.array(img, dtype=np.float32) / 255.0\n",
    "        return img_array\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image {filepath}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def predict_single_image(model, image_path, image_size=(224, 224), threshold=0.5):\n",
    "    \"\"\"Predict quality for a single image\"\"\"\n",
    "    img = load_and_preprocess_image(image_path, image_size)\n",
    "    if img is None:\n",
    "        return None\n",
    "    \n",
    "    img_batch = np.expand_dims(img, axis=0)\n",
    "    prediction = model.predict(img_batch, verbose=0)[0][0]\n",
    "    \n",
    "    quality_label = \"GOOD\" if prediction >= threshold else \"BAD\"\n",
    "    confidence = prediction if prediction >= 0.5 else (1 - prediction)\n",
    "    \n",
    "    return {\n",
    "        'quality_score': prediction,\n",
    "        'quality_label': quality_label,\n",
    "        'confidence': confidence\n",
    "    }\n",
    "\n",
    "\n",
    "def visualize_prediction(image_path, prediction_result):\n",
    "    \"\"\"Visualize image with prediction\"\"\"\n",
    "    img = Image.open(image_path)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Add prediction text\n",
    "    score = prediction_result['quality_score']\n",
    "    label = prediction_result['quality_label']\n",
    "    confidence = prediction_result['confidence']\n",
    "    \n",
    "    color = 'green' if label == 'GOOD' else 'red'\n",
    "    title = f\"Quality: {label}\\nScore: {score:.4f} | Confidence: {confidence:.2%}\"\n",
    "    \n",
    "    plt.title(title, fontsize=14, weight='bold', color=color, pad=10)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def calculate_metrics(predictions, labels, threshold=0.5):\n",
    "    \"\"\"Calculate evaluation metrics\"\"\"\n",
    "    predictions = np.array(predictions)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    # Regression metrics\n",
    "    errors = np.abs(predictions - labels)\n",
    "    mae = np.mean(errors)\n",
    "    rmse = np.sqrt(np.mean((predictions - labels) ** 2))\n",
    "    \n",
    "    # Classification metrics\n",
    "    pred_classes = (predictions >= threshold).astype(int)\n",
    "    true_classes = (labels >= threshold).astype(int)\n",
    "    \n",
    "    accuracy = np.mean(pred_classes == true_classes)\n",
    "    tp = np.sum((pred_classes == 1) & (true_classes == 1))\n",
    "    fp = np.sum((pred_classes == 1) & (true_classes == 0))\n",
    "    fn = np.sum((pred_classes == 0) & (true_classes == 1))\n",
    "    tn = np.sum((pred_classes == 0) & (true_classes == 0))\n",
    "    \n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'mae': mae,\n",
    "        'rmse': rmse,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'confusion_matrix': {'tp': tp, 'fp': fp, 'tn': tn, 'fn': fn}\n",
    "    }\n",
    "\n",
    "print(\"✓ Helper functions loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "option_select"
   },
   "source": [
    "## 4. Choose Testing Mode\n",
    "\n",
    "Select one of the following:\n",
    "- **Option A**: Test on a single RGB image\n",
    "- **Option B**: Test on multiple images (ZIP file)\n",
    "- **Option C**: Evaluate on test set (CSV with filepaths and labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "option_a"
   },
   "source": [
    "### Option A: Test Single Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "option_a_code"
   },
   "outputs": [],
   "source": [
    "# Upload a single RGB SOM image\n",
    "print(\"Please upload a single RGB SOM image (PNG format)...\")\n",
    "uploaded_img = files.upload()\n",
    "\n",
    "# Get the uploaded image filename\n",
    "img_filename = list(uploaded_img.keys())[0]\n",
    "print(f\"\\n✓ Image uploaded: {img_filename}\")\n",
    "\n",
    "# Predict\n",
    "print(\"\\nMaking prediction...\")\n",
    "result = predict_single_image(model, img_filename)\n",
    "\n",
    "if result:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"PREDICTION RESULT\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Quality Score: {result['quality_score']:.6f}\")\n",
    "    print(f\"Quality Label: {result['quality_label']}\")\n",
    "    print(f\"Confidence: {result['confidence']:.2%}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Visualize\n",
    "    visualize_prediction(img_filename, result)\n",
    "else:\n",
    "    print(\"✗ Failed to process image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "option_b"
   },
   "source": [
    "### Option B: Test Multiple Images (ZIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "option_b_upload"
   },
   "outputs": [],
   "source": [
    "# Upload a ZIP file containing RGB images\n",
    "print(\"Please upload a ZIP file containing RGB SOM images...\")\n",
    "uploaded_zip = files.upload()\n",
    "\n",
    "# Get the uploaded ZIP filename\n",
    "zip_filename = list(uploaded_zip.keys())[0]\n",
    "print(f\"\\n✓ ZIP uploaded: {zip_filename}\")\n",
    "\n",
    "# Extract ZIP\n",
    "extract_dir = \"uploaded_images\"\n",
    "os.makedirs(extract_dir, exist_ok=True)\n",
    "\n",
    "with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_dir)\n",
    "\n",
    "print(f\"✓ Extracted to: {extract_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "option_b_predict"
   },
   "outputs": [],
   "source": [
    "# Find all PNG images\n",
    "image_files = []\n",
    "for root, dirs, files in os.walk(extract_dir):\n",
    "    for file in files:\n",
    "        if file.endswith('.png'):\n",
    "            image_files.append(os.path.join(root, file))\n",
    "\n",
    "print(f\"Found {len(image_files)} PNG images\\n\")\n",
    "\n",
    "if len(image_files) == 0:\n",
    "    print(\"✗ No PNG images found in ZIP file\")\n",
    "else:\n",
    "    # Predict on all images\n",
    "    results = []\n",
    "    \n",
    "    print(\"Making predictions...\")\n",
    "    for i, img_path in enumerate(image_files):\n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"  Progress: {i+1}/{len(image_files)}\")\n",
    "        \n",
    "        result = predict_single_image(model, img_path)\n",
    "        if result:\n",
    "            results.append({\n",
    "                'filename': os.path.basename(img_path),\n",
    "                'filepath': img_path,\n",
    "                'quality_score': result['quality_score'],\n",
    "                'quality_label': result['quality_label'],\n",
    "                'confidence': result['confidence']\n",
    "            })\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df = results_df.sort_values('quality_score', ascending=False)\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"PREDICTION SUMMARY\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    good_count = (results_df['quality_score'] >= 0.5).sum()\n",
    "    bad_count = (results_df['quality_score'] < 0.5).sum()\n",
    "    \n",
    "    print(f\"Total predictions: {len(results_df)}\")\n",
    "    print(f\"  Good (score >= 0.5): {good_count}\")\n",
    "    print(f\"  Bad (score < 0.5): {bad_count}\")\n",
    "    \n",
    "    print(f\"\\nTop 10 Best Quality Maps:\")\n",
    "    print(results_df.head(10)[['filename', 'quality_score', 'quality_label', 'confidence']].to_string(index=False))\n",
    "    \n",
    "    print(f\"\\nTop 10 Worst Quality Maps:\")\n",
    "    print(results_df.tail(10)[['filename', 'quality_score', 'quality_label', 'confidence']].to_string(index=False))\n",
    "    \n",
    "    # Save results\n",
    "    output_csv = \"predictions.csv\"\n",
    "    results_df.to_csv(output_csv, index=False)\n",
    "    print(f\"\\n✓ Predictions saved to: {output_csv}\")\n",
    "    \n",
    "    # Download results\n",
    "    print(\"\\nDownloading predictions.csv...\")\n",
    "    files.download(output_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "option_b_visualize"
   },
   "outputs": [],
   "source": [
    "# Visualize sample predictions\n",
    "print(\"Visualizing sample predictions...\\n\")\n",
    "\n",
    "# Show top 3 best and top 3 worst\n",
    "sample_indices = list(results_df.head(3).index) + list(results_df.tail(3).index)\n",
    "\n",
    "for idx in sample_indices:\n",
    "    row = results_df.iloc[idx]\n",
    "    result = {\n",
    "        'quality_score': row['quality_score'],\n",
    "        'quality_label': row['quality_label'],\n",
    "        'confidence': row['confidence']\n",
    "    }\n",
    "    print(f\"\\nImage: {row['filename']}\")\n",
    "    visualize_prediction(row['filepath'], result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "option_c"
   },
   "source": [
    "### Option C: Evaluate on Test Set (CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "option_c_upload"
   },
   "outputs": [],
   "source": [
    "# Upload test set CSV\n",
    "print(\"Please upload test set CSV file (must have 'filepath' and 'quality_score' columns)...\")\n",
    "uploaded_csv = files.upload()\n",
    "\n",
    "# Get the uploaded CSV filename\n",
    "csv_filename = list(uploaded_csv.keys())[0]\n",
    "print(f\"\\n✓ CSV uploaded: {csv_filename}\")\n",
    "\n",
    "# Load test set\n",
    "test_df = pd.read_csv(csv_filename)\n",
    "print(f\"\\nTest set size: {len(test_df)} samples\")\n",
    "print(f\"Columns: {list(test_df.columns)}\")\n",
    "\n",
    "if 'filepath' not in test_df.columns or 'quality_score' not in test_df.columns:\n",
    "    print(\"\\n✗ Error: CSV must contain 'filepath' and 'quality_score' columns\")\n",
    "else:\n",
    "    print(\"\\n✓ Test set loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "option_c_upload_images"
   },
   "outputs": [],
   "source": [
    "# Upload images referenced in the CSV\n",
    "print(\"Please upload a ZIP file containing the images referenced in the CSV...\")\n",
    "uploaded_img_zip = files.upload()\n",
    "\n",
    "# Get the uploaded ZIP filename\n",
    "img_zip_filename = list(uploaded_img_zip.keys())[0]\n",
    "print(f\"\\n✓ ZIP uploaded: {img_zip_filename}\")\n",
    "\n",
    "# Extract ZIP\n",
    "images_dir = \"test_images\"\n",
    "os.makedirs(images_dir, exist_ok=True)\n",
    "\n",
    "with zipfile.ZipFile(img_zip_filename, 'r') as zip_ref:\n",
    "    zip_ref.extractall(images_dir)\n",
    "\n",
    "print(f\"✓ Extracted to: {images_dir}\")\n",
    "\n",
    "# Update filepaths in test_df to point to extracted location\n",
    "# Assumes images are directly in the ZIP root\n",
    "test_df['local_filepath'] = test_df['filepath'].apply(\n",
    "    lambda x: os.path.join(images_dir, os.path.basename(x))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "option_c_evaluate"
   },
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"EVALUATING ON TEST SET\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Load images and labels\n",
    "images = []\n",
    "labels = []\n",
    "valid_indices = []\n",
    "\n",
    "print(\"Loading images...\")\n",
    "for idx, row in test_df.iterrows():\n",
    "    img = load_and_preprocess_image(row['local_filepath'])\n",
    "    if img is not None:\n",
    "        images.append(img)\n",
    "        labels.append(row['quality_score'])\n",
    "        valid_indices.append(idx)\n",
    "    else:\n",
    "        print(f\"  Warning: Could not load {row['local_filepath']}\")\n",
    "\n",
    "images = np.array(images)\n",
    "labels = np.array(labels)\n",
    "\n",
    "print(f\"\\nLoaded {len(images)} images\")\n",
    "\n",
    "# Predict\n",
    "print(\"\\nMaking predictions...\")\n",
    "predictions = model.predict(images, verbose=1)\n",
    "predictions = predictions.flatten()\n",
    "\n",
    "# Calculate metrics\n",
    "metrics = calculate_metrics(predictions, labels)\n",
    "\n",
    "# Print results\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"EVALUATION RESULTS\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "print(\"Regression Metrics:\")\n",
    "print(f\"  Mean Absolute Error (MAE): {metrics['mae']:.6f}\")\n",
    "print(f\"  Root Mean Squared Error (RMSE): {metrics['rmse']:.6f}\")\n",
    "\n",
    "print(f\"\\nClassification Metrics (threshold=0.5):\")\n",
    "print(f\"  Accuracy: {metrics['accuracy']:.4f} ({metrics['accuracy']*100:.2f}%)\")\n",
    "print(f\"  Precision: {metrics['precision']:.4f}\")\n",
    "print(f\"  Recall: {metrics['recall']:.4f}\")\n",
    "print(f\"  F1-Score: {metrics['f1']:.4f}\")\n",
    "\n",
    "cm = metrics['confusion_matrix']\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(f\"  True Positives: {cm['tp']}\")\n",
    "print(f\"  False Positives: {cm['fp']}\")\n",
    "print(f\"  True Negatives: {cm['tn']}\")\n",
    "print(f\"  False Negatives: {cm['fn']}\")\n",
    "\n",
    "# Show sample predictions\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"SAMPLE PREDICTIONS\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "for i in range(min(10, len(predictions))):\n",
    "    pred_label = \"GOOD\" if predictions[i] >= 0.5 else \"BAD\"\n",
    "    true_label = \"GOOD\" if labels[i] >= 0.5 else \"BAD\"\n",
    "    match = \"✓\" if pred_label == true_label else \"✗\"\n",
    "    print(f\"{match} Sample {i+1}: Predicted={predictions[i]:.4f} ({pred_label}), True={labels[i]:.1f} ({true_label})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "option_c_visualize"
   },
   "outputs": [],
   "source": [
    "# Visualize prediction distribution\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot 1: Prediction vs True label scatter\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(labels, predictions, alpha=0.5)\n",
    "plt.plot([0, 1], [0, 1], 'r--', label='Perfect prediction')\n",
    "plt.axhline(y=0.5, color='orange', linestyle='--', alpha=0.5, label='Threshold')\n",
    "plt.axvline(x=0.5, color='orange', linestyle='--', alpha=0.5)\n",
    "plt.xlabel('True Quality Score')\n",
    "plt.ylabel('Predicted Quality Score')\n",
    "plt.title('Predictions vs True Labels')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Prediction distribution\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(predictions[labels == 1.0], bins=20, alpha=0.5, label='True GOOD', color='green')\n",
    "plt.hist(predictions[labels == 0.0], bins=20, alpha=0.5, label='True BAD', color='red')\n",
    "plt.axvline(x=0.5, color='black', linestyle='--', label='Threshold')\n",
    "plt.xlabel('Predicted Quality Score')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Prediction Distribution')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "conclusion"
   },
   "source": [
    "## 5. Model Performance Summary\n",
    "\n",
    "**The Eye** CNN Model Capabilities:\n",
    "- Predicts SOM quality from RGB composite visualizations\n",
    "- Outputs quality score (0.0 = BAD, 1.0 = GOOD)\n",
    "- High precision (avoids false positives)\n",
    "- Can be integrated into evolutionary algorithms for real-time quality assessment\n",
    "\n",
    "### RGB Channel Interpretation:\n",
    "- **Red**: U-Matrix (cluster boundaries - dark=similar, yellow=boundaries)\n",
    "- **Green**: Distance Map (quantization error)\n",
    "- **Blue**: Dead Neurons Map (unused neurons)\n",
    "\n",
    "### Quality Indicators:\n",
    "- Clear cluster boundaries in U-Matrix\n",
    "- Low quantization error (distance map)\n",
    "- Few dead neurons\n",
    "- Well-organized topology\n",
    "\n",
    "---\n",
    "\n",
    "**For more information:**\n",
    "- GitHub: NexusSom Project\n",
    "- Model trained on evolutionary algorithm (EA) results\n",
    "- Fixed [0, 1.0] normalization for consistent visual interpretation"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}