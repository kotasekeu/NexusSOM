{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# The Eye - SOM Quality Prediction CNN\n",
    "\n",
    "This notebook allows you to test \"The Eye\" CNN model for predicting Self-Organizing Map (SOM) quality.\n",
    "\n",
    "**The Eye** is a CNN trained to predict SOM quality by analyzing RGB composite visualizations:\n",
    "- **Red Channel**: U-Matrix (cluster boundaries)\n",
    "- **Green Channel**: Distance Map (quantization error)\n",
    "- **Blue Channel**: Dead Neurons Map\n",
    "\n",
    "## Quick Start\n",
    "1. Upload your trained model (.keras file)\n",
    "2. Upload RGB SOM maps or test set CSV\n",
    "3. Run evaluation cells\n",
    "4. Download predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "imports",
    "ExecuteTime": {
     "end_time": "2026-01-13T17:20:25.043287Z",
     "start_time": "2026-01-13T17:20:17.654566Z"
    }
   },
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import zipfile\n",
    "import io\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Keras version: {keras.__version__}\")\n",
    "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")\n",
    "\n",
    "# Check if TensorFlow version is compatible\n",
    "tf_version = tf.__version__\n",
    "major, minor = map(int, tf_version.split('.')[:2])\n",
    "if major < 2 or (major == 2 and minor < 18):\n",
    "    print(f\"\\n⚠ WARNING: TensorFlow {tf_version} detected.\")\n",
    "    print(\"  This model requires TensorFlow 2.18+\")\n",
    "    print(\"  Please restart runtime and run the installation cell again.\")\n",
    "else:\n",
    "    print(f\"\\n✓ TensorFlow version compatible ({tf_version})\")\n",
    "\n",
    "print(\"\\n✓ Imports successful!\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.15.0\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'keras.api._v2.keras' has no attribute '__version__'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mAttributeError\u001B[39m                            Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 15\u001B[39m\n\u001B[32m     12\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mio\u001B[39;00m\n\u001B[32m     14\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mTensorFlow version: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtf.__version__\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m15\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mKeras version: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[43mkeras\u001B[49m\u001B[43m.\u001B[49m\u001B[43m__version__\u001B[49m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m     16\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mGPU Available: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtf.config.list_physical_devices(\u001B[33m'\u001B[39m\u001B[33mGPU\u001B[39m\u001B[33m'\u001B[39m)\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m     18\u001B[39m \u001B[38;5;66;03m# Check if TensorFlow version is compatible\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/OSU/Python/NexusSom/app/venv_tf/lib/python3.11/site-packages/tensorflow/python/util/lazy_loader.py:171\u001B[39m, in \u001B[36mKerasLazyLoader.__getattr__\u001B[39m\u001B[34m(self, item)\u001B[39m\n\u001B[32m    167\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(\n\u001B[32m    168\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m`\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mitem\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m` is not available with Keras 3.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    169\u001B[39m     )\n\u001B[32m    170\u001B[39m module = \u001B[38;5;28mself\u001B[39m._load()\n\u001B[32m--> \u001B[39m\u001B[32m171\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mmodule\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mitem\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mAttributeError\u001B[39m: module 'keras.api._v2.keras' has no attribute '__version__'"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upload_model"
   },
   "source": [
    "## 2. Upload Model\n",
    "\n",
    "Upload your trained `.keras` model file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upload_model_code"
   },
   "outputs": [],
   "source": [
    "model_filename = \"som_quality_standard_best.keras\"\n",
    "print(f\"\\n✓ Model uploaded: {model_filename}\")\n",
    "\n",
    "# Load the model with compatibility handling\n",
    "print(\"Loading model...\")\n",
    "try:\n",
    "    # Try loading normally first\n",
    "    model = keras.models.load_model(model_filename)\n",
    "    print(\"✓ Model loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠ Standard loading failed: {e}\")\n",
    "    print(\"\\nTrying compatibility mode (loading weights only)...\")\n",
    "    \n",
    "    try:\n",
    "        # Alternative: Load with compile=False to skip optimizer state\n",
    "        model = keras.models.load_model(model_filename, compile=False)\n",
    "        \n",
    "        # Recompile with simple optimizer\n",
    "        model.compile(\n",
    "            optimizer='adam',\n",
    "            loss='mse',\n",
    "            metrics=['mae']\n",
    "        )\n",
    "        print(\"✓ Model loaded in compatibility mode (weights only)\")\n",
    "        print(\"  Note: Original optimizer state not loaded, using default Adam\")\n",
    "    except Exception as e2:\n",
    "        print(f\"✗ Compatibility mode also failed: {e2}\")\n",
    "        print(\"\\nPlease ensure you have TensorFlow 2.18+ installed:\")\n",
    "        print(\"  !pip install --upgrade tensorflow>=2.18.0\")\n",
    "        raise\n",
    "\n",
    "# Show model summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL ARCHITECTURE\")\n",
    "print(\"=\"*80)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "helper_functions"
   },
   "source": [
    "## 3. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "helper_code"
   },
   "outputs": [],
   "source": [
    "def load_and_preprocess_image(filepath, image_size=(224, 224)):\n",
    "    \"\"\"Load and preprocess a single image\"\"\"\n",
    "    try:\n",
    "        img = Image.open(filepath).convert('RGB')\n",
    "        img = img.resize(image_size, Image.LANCZOS)\n",
    "        img_array = np.array(img, dtype=np.float32) / 255.0\n",
    "        return img_array\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image {filepath}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def predict_single_image(model, image_path, image_size=(224, 224), threshold=0.5):\n",
    "    \"\"\"Predict quality for a single image\"\"\"\n",
    "    img = load_and_preprocess_image(image_path, image_size)\n",
    "    if img is None:\n",
    "        return None\n",
    "    \n",
    "    img_batch = np.expand_dims(img, axis=0)\n",
    "    prediction = model.predict(img_batch, verbose=0)[0][0]\n",
    "    \n",
    "    quality_label = \"GOOD\" if prediction >= threshold else \"BAD\"\n",
    "    confidence = prediction if prediction >= 0.5 else (1 - prediction)\n",
    "    \n",
    "    return {\n",
    "        'quality_score': prediction,\n",
    "        'quality_label': quality_label,\n",
    "        'confidence': confidence\n",
    "    }\n",
    "\n",
    "\n",
    "def visualize_prediction(image_path, prediction_result):\n",
    "    \"\"\"Visualize image with prediction\"\"\"\n",
    "    img = Image.open(image_path)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Add prediction text\n",
    "    score = prediction_result['quality_score']\n",
    "    label = prediction_result['quality_label']\n",
    "    confidence = prediction_result['confidence']\n",
    "    \n",
    "    color = 'green' if label == 'GOOD' else 'red'\n",
    "    title = f\"Quality: {label}\\nScore: {score:.4f} | Confidence: {confidence:.2%}\"\n",
    "    \n",
    "    plt.title(title, fontsize=14, weight='bold', color=color, pad=10)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def calculate_metrics(predictions, labels, threshold=0.5):\n",
    "    \"\"\"Calculate evaluation metrics\"\"\"\n",
    "    predictions = np.array(predictions)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    # Regression metrics\n",
    "    errors = np.abs(predictions - labels)\n",
    "    mae = np.mean(errors)\n",
    "    rmse = np.sqrt(np.mean((predictions - labels) ** 2))\n",
    "    \n",
    "    # Classification metrics\n",
    "    pred_classes = (predictions >= threshold).astype(int)\n",
    "    true_classes = (labels >= threshold).astype(int)\n",
    "    \n",
    "    accuracy = np.mean(pred_classes == true_classes)\n",
    "    tp = np.sum((pred_classes == 1) & (true_classes == 1))\n",
    "    fp = np.sum((pred_classes == 1) & (true_classes == 0))\n",
    "    fn = np.sum((pred_classes == 0) & (true_classes == 1))\n",
    "    tn = np.sum((pred_classes == 0) & (true_classes == 0))\n",
    "    \n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'mae': mae,\n",
    "        'rmse': rmse,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'confusion_matrix': {'tp': tp, 'fp': fp, 'tn': tn, 'fn': fn}\n",
    "    }\n",
    "\n",
    "print(\"✓ Helper functions loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "option_select"
   },
   "source": [
    "## 4. Choose Testing Mode\n",
    "\n",
    "Select one of the following:\n",
    "- **Option A**: Test on a single RGB image\n",
    "- **Option B**: Test on multiple images (ZIP file)\n",
    "- **Option C**: Evaluate on test set (CSV with filepaths and labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "option_a"
   },
   "source": [
    "### Option A: Test Single Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "option_a_code"
   },
   "outputs": [],
   "source": [
    "# Upload a single RGB SOM image\n",
    "print(\"⚠ Not in Colab - please set img_filename manually\")\n",
    "img_filename = \"bad.png\"\n",
    "\n",
    "# Predict\n",
    "print(\"\\nMaking prediction...\")\n",
    "result = predict_single_image(model, img_filename)\n",
    "\n",
    "if result:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"PREDICTION RESULT\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Quality Score: {result['quality_score']:.6f}\")\n",
    "    print(f\"Quality Label: {result['quality_label']}\")\n",
    "    print(f\"Confidence: {result['confidence']:.2%}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Visualize\n",
    "    visualize_prediction(img_filename, result)\n",
    "else:\n",
    "    print(\"✗ Failed to process image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "option_b"
   },
   "source": [
    "### Option B: Test Multiple Images (ZIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "option_b_upload"
   },
   "outputs": [],
   "source": "# Upload a ZIP file containing RGB images\nif not IN_COLAB:\n    print(\"⚠ Not in Colab - please set zip_filename manually\")\n    zip_filename = \"images.zip\"  # Change this to your ZIP path\nelse:\n    print(\"Please upload a ZIP file containing RGB SOM images...\")\n    uploaded_zip = files.upload()\n    \n    # Get the uploaded ZIP filename\n    zip_filename = list(uploaded_zip.keys())[0]\n    print(f\"\\n✓ ZIP uploaded: {zip_filename}\")\n\n# Extract ZIP\nextract_dir = \"uploaded_images\"\nos.makedirs(extract_dir, exist_ok=True)\n\nwith zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n    zip_ref.extractall(extract_dir)\n\nprint(f\"✓ Extracted to: {extract_dir}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "option_b_predict"
   },
   "outputs": [],
   "source": "# Find all PNG images\nimage_files = []\nfor root, dirs, files_list in os.walk(extract_dir):\n    for file in files_list:\n        if file.endswith('.png'):\n            image_files.append(os.path.join(root, file))\n\nprint(f\"Found {len(image_files)} PNG images\\n\")\n\nif len(image_files) == 0:\n    print(\"✗ No PNG images found in ZIP file\")\nelse:\n    # Predict on all images\n    results = []\n    \n    print(\"Making predictions...\")\n    for i, img_path in enumerate(image_files):\n        if (i + 1) % 10 == 0:\n            print(f\"  Progress: {i+1}/{len(image_files)}\")\n        \n        result = predict_single_image(model, img_path)\n        if result:\n            results.append({\n                'filename': os.path.basename(img_path),\n                'filepath': img_path,\n                'quality_score': result['quality_score'],\n                'quality_label': result['quality_label'],\n                'confidence': result['confidence']\n            })\n    \n    # Convert to DataFrame\n    results_df = pd.DataFrame(results)\n    results_df = results_df.sort_values('quality_score', ascending=False)\n    \n    # Summary\n    print(f\"\\n{'='*80}\")\n    print(\"PREDICTION SUMMARY\")\n    print(f\"{'='*80}\\n\")\n    \n    good_count = (results_df['quality_score'] >= 0.5).sum()\n    bad_count = (results_df['quality_score'] < 0.5).sum()\n    \n    print(f\"Total predictions: {len(results_df)}\")\n    print(f\"  Good (score >= 0.5): {good_count}\")\n    print(f\"  Bad (score < 0.5): {bad_count}\")\n    \n    print(f\"\\nTop 10 Best Quality Maps:\")\n    print(results_df.head(10)[['filename', 'quality_score', 'quality_label', 'confidence']].to_string(index=False))\n    \n    print(f\"\\nTop 10 Worst Quality Maps:\")\n    print(results_df.tail(10)[['filename', 'quality_score', 'quality_label', 'confidence']].to_string(index=False))\n    \n    # Save results\n    output_csv = \"predictions.csv\"\n    results_df.to_csv(output_csv, index=False)\n    print(f\"\\n✓ Predictions saved to: {output_csv}\")\n    \n    # Download results\n    if IN_COLAB:\n        print(\"\\nDownloading predictions.csv...\")\n        files.download(output_csv)\n    else:\n        print(f\"\\n✓ Results saved locally to {output_csv}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "option_b_visualize"
   },
   "outputs": [],
   "source": [
    "# Visualize sample predictions\n",
    "print(\"Visualizing sample predictions...\\n\")\n",
    "\n",
    "# Show top 3 best and top 3 worst\n",
    "sample_indices = list(results_df.head(3).index) + list(results_df.tail(3).index)\n",
    "\n",
    "for idx in sample_indices:\n",
    "    row = results_df.iloc[idx]\n",
    "    result = {\n",
    "        'quality_score': row['quality_score'],\n",
    "        'quality_label': row['quality_label'],\n",
    "        'confidence': row['confidence']\n",
    "    }\n",
    "    print(f\"\\nImage: {row['filename']}\")\n",
    "    visualize_prediction(row['filepath'], result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "option_c"
   },
   "source": [
    "### Option C: Evaluate on Test Set (CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "option_c_upload"
   },
   "outputs": [],
   "source": "# Upload test set CSV\nif not IN_COLAB:\n    print(\"⚠ Not in Colab - please set csv_filename manually\")\n    csv_filename = \"test_set.csv\"  # Change this to your CSV path\nelse:\n    print(\"Please upload test set CSV file (must have 'filepath' and 'quality_score' columns)...\")\n    uploaded_csv = files.upload()\n    \n    # Get the uploaded CSV filename\n    csv_filename = list(uploaded_csv.keys())[0]\n    print(f\"\\n✓ CSV uploaded: {csv_filename}\")\n\n# Load test set\ntest_df = pd.read_csv(csv_filename)\nprint(f\"\\nTest set size: {len(test_df)} samples\")\nprint(f\"Columns: {list(test_df.columns)}\")\n\nif 'filepath' not in test_df.columns or 'quality_score' not in test_df.columns:\n    print(\"\\n✗ Error: CSV must contain 'filepath' and 'quality_score' columns\")\nelse:\n    print(\"\\n✓ Test set loaded successfully!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "option_c_upload_images"
   },
   "outputs": [],
   "source": "# Upload images referenced in the CSV\nif not IN_COLAB:\n    print(\"⚠ Not in Colab - please set img_zip_filename manually\")\n    img_zip_filename = \"test_images.zip\"  # Change this to your ZIP path\nelse:\n    print(\"Please upload a ZIP file containing the images referenced in the CSV...\")\n    uploaded_img_zip = files.upload()\n    \n    # Get the uploaded ZIP filename\n    img_zip_filename = list(uploaded_img_zip.keys())[0]\n    print(f\"\\n✓ ZIP uploaded: {img_zip_filename}\")\n\n# Extract ZIP\nimages_dir = \"test_images\"\nos.makedirs(images_dir, exist_ok=True)\n\nwith zipfile.ZipFile(img_zip_filename, 'r') as zip_ref:\n    zip_ref.extractall(images_dir)\n\nprint(f\"✓ Extracted to: {images_dir}\")\n\n# Update filepaths in test_df to point to extracted location\n# Assumes images are directly in the ZIP root\ntest_df['local_filepath'] = test_df['filepath'].apply(\n    lambda x: os.path.join(images_dir, os.path.basename(x))\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "option_c_evaluate"
   },
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"EVALUATING ON TEST SET\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Load images and labels\n",
    "images = []\n",
    "labels = []\n",
    "valid_indices = []\n",
    "\n",
    "print(\"Loading images...\")\n",
    "for idx, row in test_df.iterrows():\n",
    "    img = load_and_preprocess_image(row['local_filepath'])\n",
    "    if img is not None:\n",
    "        images.append(img)\n",
    "        labels.append(row['quality_score'])\n",
    "        valid_indices.append(idx)\n",
    "    else:\n",
    "        print(f\"  Warning: Could not load {row['local_filepath']}\")\n",
    "\n",
    "images = np.array(images)\n",
    "labels = np.array(labels)\n",
    "\n",
    "print(f\"\\nLoaded {len(images)} images\")\n",
    "\n",
    "# Predict\n",
    "print(\"\\nMaking predictions...\")\n",
    "predictions = model.predict(images, verbose=1)\n",
    "predictions = predictions.flatten()\n",
    "\n",
    "# Calculate metrics\n",
    "metrics = calculate_metrics(predictions, labels)\n",
    "\n",
    "# Print results\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"EVALUATION RESULTS\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "print(\"Regression Metrics:\")\n",
    "print(f\"  Mean Absolute Error (MAE): {metrics['mae']:.6f}\")\n",
    "print(f\"  Root Mean Squared Error (RMSE): {metrics['rmse']:.6f}\")\n",
    "\n",
    "print(f\"\\nClassification Metrics (threshold=0.5):\")\n",
    "print(f\"  Accuracy: {metrics['accuracy']:.4f} ({metrics['accuracy']*100:.2f}%)\")\n",
    "print(f\"  Precision: {metrics['precision']:.4f}\")\n",
    "print(f\"  Recall: {metrics['recall']:.4f}\")\n",
    "print(f\"  F1-Score: {metrics['f1']:.4f}\")\n",
    "\n",
    "cm = metrics['confusion_matrix']\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(f\"  True Positives: {cm['tp']}\")\n",
    "print(f\"  False Positives: {cm['fp']}\")\n",
    "print(f\"  True Negatives: {cm['tn']}\")\n",
    "print(f\"  False Negatives: {cm['fn']}\")\n",
    "\n",
    "# Show sample predictions\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"SAMPLE PREDICTIONS\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "for i in range(min(10, len(predictions))):\n",
    "    pred_label = \"GOOD\" if predictions[i] >= 0.5 else \"BAD\"\n",
    "    true_label = \"GOOD\" if labels[i] >= 0.5 else \"BAD\"\n",
    "    match = \"✓\" if pred_label == true_label else \"✗\"\n",
    "    print(f\"{match} Sample {i+1}: Predicted={predictions[i]:.4f} ({pred_label}), True={labels[i]:.1f} ({true_label})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "option_c_visualize"
   },
   "outputs": [],
   "source": [
    "# Visualize prediction distribution\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot 1: Prediction vs True label scatter\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(labels, predictions, alpha=0.5)\n",
    "plt.plot([0, 1], [0, 1], 'r--', label='Perfect prediction')\n",
    "plt.axhline(y=0.5, color='orange', linestyle='--', alpha=0.5, label='Threshold')\n",
    "plt.axvline(x=0.5, color='orange', linestyle='--', alpha=0.5)\n",
    "plt.xlabel('True Quality Score')\n",
    "plt.ylabel('Predicted Quality Score')\n",
    "plt.title('Predictions vs True Labels')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Prediction distribution\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(predictions[labels == 1.0], bins=20, alpha=0.5, label='True GOOD', color='green')\n",
    "plt.hist(predictions[labels == 0.0], bins=20, alpha=0.5, label='True BAD', color='red')\n",
    "plt.axvline(x=0.5, color='black', linestyle='--', label='Threshold')\n",
    "plt.xlabel('Predicted Quality Score')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Prediction Distribution')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "conclusion"
   },
   "source": [
    "## 5. Model Performance Summary\n",
    "\n",
    "**The Eye** CNN Model Capabilities:\n",
    "- Predicts SOM quality from RGB composite visualizations\n",
    "- Outputs quality score (0.0 = BAD, 1.0 = GOOD)\n",
    "- High precision (avoids false positives)\n",
    "- Can be integrated into evolutionary algorithms for real-time quality assessment\n",
    "\n",
    "### RGB Channel Interpretation:\n",
    "- **Red**: U-Matrix (cluster boundaries - dark=similar, yellow=boundaries)\n",
    "- **Green**: Distance Map (quantization error)\n",
    "- **Blue**: Dead Neurons Map (unused neurons)\n",
    "\n",
    "### Quality Indicators:\n",
    "- Clear cluster boundaries in U-Matrix\n",
    "- Low quantization error (distance map)\n",
    "- Few dead neurons\n",
    "- Well-organized topology\n",
    "\n",
    "---\n",
    "\n",
    "**For more information:**\n",
    "- GitHub: NexusSom Project\n",
    "- Model trained on evolutionary algorithm (EA) results\n",
    "- Fixed [0, 1.0] normalization for consistent visual interpretation"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
